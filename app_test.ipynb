{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from htmldate import find_date\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "from tqdm import tqdm\n",
    "from textblob import TextBlob\n",
    "import openai\n",
    "import multiprocessing\n",
    "from time import time\n",
    "\n",
    "from html_extractor import *\n",
    "from get_suburls import *\n",
    "from openai_func import *\n",
    "from get_date import *\n",
    "from parallel import *\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "from keyword_extraction import keyword_extractor_paragraph as kep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting sub urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.68it/s]\n",
      "100%|██████████| 5/5 [00:06<00:00,  1.29s/it]\n",
      "100%|██████████| 66/66 [00:23<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed Fetch: 0\n",
      "Splits: 4\n",
      "Tree size: 181\n"
     ]
    }
   ],
   "source": [
    "urls_list = [\"https://www.khaleejtimes.com\" , \"https://www.indiatoday.in\"]\n",
    "urls_list_str = \",\".join(urls_list)\n",
    "\n",
    "keywords = \"gaza,israel,hamas,idf\"\n",
    "\n",
    "scraper = WebScraper2(sub_url_size = 3 , keywords = keywords)\n",
    "                        # Integration with DB will make it faster in future, as fetching is much faster than scrapping.\n",
    "inside_urls, failed_fetch, sub_url_size, total_size = scraper.get_suburls2(urls_list_str)\n",
    "\n",
    "# print(\"Inside URLs:\", inside_urls)\n",
    "print(\"Failed Fetch:\", failed_fetch)\n",
    "print(\"Splits:\", len(inside_urls))\n",
    "print(\"Tree size:\", total_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining sub urls into one single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181\n"
     ]
    }
   ],
   "source": [
    "website_urls = [item for sublist in list(inside_urls.values()) for item in sublist]\n",
    "print(len(website_urls))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Date Fetching\n",
    "\n",
    "Need to integrate Mongo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://www.indiatoday.in/topic/israel', '12-09-2019')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_date_from_url(website_urls[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating {url : html content} dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181/181 [00:59<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "url_html_extracted = get_html(website_urls)\n",
    "# url_html_extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword extraction performed on above dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:00<00:00, 8663.04it/s]\n"
     ]
    }
   ],
   "source": [
    "url_extracted_html = kep(website_content = url_html_extracted[0], keywords = keywords, filter_by_amount = 60)\n",
    "\n",
    "# url_extracted_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting dictionary to list of tuple pairs, for implementation of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list = [(key,value[:2000]) for key, value in url_extracted_html.items()] # 1000 is temporary until tokenier function is not set up\n",
    "# content_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutting the above list fo batches of batch size MAX_CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_CONTENT = 5\n",
    "\n",
    "content_list_complete = []\n",
    "\n",
    "iterations = len(content_list) // MAX_CONTENT\n",
    "\n",
    "\n",
    "for i in range(iterations):\n",
    "    sub_content_list = content_list[MAX_CONTENT * i: MAX_CONTENT * (i + 1)]\n",
    "    content_list_complete.append(sub_content_list)\n",
    "\n",
    "# Handle remaining elements after the loop\n",
    "remaining_elements = content_list[MAX_CONTENT * iterations:]\n",
    "if remaining_elements:\n",
    "    iterations += 1\n",
    "    content_list_complete.append(remaining_elements)\n",
    "\n",
    "len(content_list_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Openai's api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non parallel execution of 1 api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 out of 24 completed \n",
      "Batch 2 out of 24 completed \n",
      "Batch 3 out of 24 completed \n",
      "Batch 4 out of 24 completed \n",
      "Batch 5 out of 24 completed \n",
      "Batch 6 out of 24 completed \n",
      "Batch 7 out of 24 completed \n",
      "Batch 8 out of 24 completed \n",
      "Batch 9 out of 24 completed \n",
      "Batch 10 out of 24 completed \n",
      "Executed in 216.86s\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "start = time()\n",
    "\n",
    "question = \"Summary of situation in gaza\"\n",
    "\n",
    "response_complete = ''\n",
    "for data_idx in range(10):\n",
    "\n",
    "    prompt = f\"\"\" \n",
    "        Data is in the form of tuples inside list: {content_list_complete[data_idx]} \\n\\n\\n \n",
    "        Question: {question} \\n\\n\\n\n",
    "        Method of reply: 100 - 200 word sentences, clear reply,\n",
    "        provide url if neccessary.\n",
    "        \"\"\"\n",
    "    \n",
    "    if data_idx % 6 == 0:\n",
    "        sleep(20)\n",
    "\n",
    "    response = get_completion(prompt)\n",
    "    response_complete += response + \"\\n\\n\"\n",
    "    print(f\"Batch {data_idx + 1} out of {iterations} completed \")\n",
    "\n",
    "end = time()\n",
    "\n",
    "print(f\"Executed in {end-start:.2f}s\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Execution for 2 api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 - 2 executed out of 24\n",
      "Batch 3 - 4 executed out of 24\n",
      "Batch 5 - 6 executed out of 24\n",
      "Batch 7 - 8 executed out of 24\n",
      "Batch 9 - 10 executed out of 24\n",
      "Executed in 98.97s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "complete_result_of_openai = \"\"\n",
    "question_to_pass = \"status of war in gaza\"\n",
    "\n",
    "count = 0\n",
    "\n",
    "NUM_OF_API = 2\n",
    "\n",
    "for i in range(0, 10, NUM_OF_API):\n",
    "\n",
    "    if count % 6 == 0:\n",
    "        sleep(20)\n",
    "    \n",
    "\n",
    "    result = ''\n",
    "\n",
    "    result_queue1 = multiprocessing.Queue()\n",
    "    result_queue2 = multiprocessing.Queue()\n",
    "\n",
    "\n",
    "    process1 = multiprocessing.Process(target=gpt1, args=(question_to_pass, content_list_complete, i, result_queue1))\n",
    "    process2 = multiprocessing.Process(target=gpt2, args=(question_to_pass, content_list_complete, i+1, result_queue2))\n",
    "\n",
    "\n",
    "    # Start processes\n",
    "    process1.start()\n",
    "    process2.start()\n",
    "\n",
    "    # Wait for processes to finish\n",
    "    process1.join()\n",
    "    process2.join()\n",
    "\n",
    "\n",
    "    result1 = result_queue1.get()\n",
    "    result2 = result_queue2.get()\n",
    "\n",
    "    # Rest of your code remains unchanged\n",
    "    result = result1 + \"\\n\\n\" + result2 + \"\\n\\n\"\n",
    "    complete_result_of_openai += result\n",
    "\n",
    "    print(f\"Batch {i+1} - {i+NUM_OF_API} executed out of {len(content_list_complete)}\")\n",
    "    count += 1\n",
    "complete_result_of_openai\n",
    "\n",
    "end = time()\n",
    "\n",
    "print(f\"Executed in {end-start:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.204081632653061"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "216/98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = get_completion(f\"Provide Detailed Summary of {complete_result_of_openai}\")\n",
    "# response3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Output_gaza_parallel.txt\" , \"w\") as f:\n",
    "    f.write(complete_result_of_openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Output_gaza_parallel_summary.txt\" , \"w\") as f:\n",
    "    f.write(complete_result_of_openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Output_gaza.txt\" , \"w\") as f:\n",
    "    f.write(response_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Output_gaza_summary.txt\" , \"w\") as f:\n",
    "    f.write(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'search_metadata': {'id': '65e3f179ca968fce70b728f5',\n",
       "  'status': 'Success',\n",
       "  'json_endpoint': 'https://serpapi.com/searches/a1f806d995d2e248/65e3f179ca968fce70b728f5.json',\n",
       "  'created_at': '2024-03-03 03:41:45 UTC',\n",
       "  'processed_at': '2024-03-03 03:41:45 UTC',\n",
       "  'google_url': 'https://www.google.com/search?q=cat&oq=cat&num=1&sourceid=chrome&ie=UTF-8',\n",
       "  'raw_html_file': 'https://serpapi.com/searches/a1f806d995d2e248/65e3f179ca968fce70b728f5.html',\n",
       "  'total_time_taken': 1.17},\n",
       " 'search_parameters': {'engine': 'google',\n",
       "  'q': 'cat',\n",
       "  'google_domain': 'google.com',\n",
       "  'num': '1',\n",
       "  'device': 'desktop'},\n",
       " 'search_information': {'query_displayed': 'cat',\n",
       "  'organic_results_state': 'Results for exact spelling'},\n",
       " 'knowledge_graph': {'title': 'About Cat',\n",
       "  'type': 'Animal',\n",
       "  'kgmid': '/m/01yrx',\n",
       "  'knowledge_graph_search_link': 'https://www.google.com/search?kgmid=/m/01yrx&hl=en-US&hl=Cat',\n",
       "  'serpapi_knowledge_graph_search_link': 'https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&hl=en-US&kgmid=%2Fm%2F01yrx&num=1&q=cat',\n",
       "  'tabs': [{'text': 'Perspectives',\n",
       "    'link': 'https://www.google.com/search?num=1&sca_esv=b60573e6fd3d297f&q=cat&uds=AMwkrPvqGo5zoYGWt6LNDCHlwCLezar5V29lBU2bhgZbER_fUF6enDWV5lHu1zQj6ZjvfWlKox1e1p4UdKUQQ7-Dj9cd-i877UfHFnn4n32RpE5Dz7VsX4U7iegJY7pyBVAqoQTkyNRh&udm=4&sa=X&ved=2ahUKEwjnnKCblteEAxX2EFkFHbsaD7cQs6gLegQIGBAB',\n",
       "    'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&num=1&q=cat'},\n",
       "   {'text': 'Breeds',\n",
       "    'link': 'https://www.google.com/search?num=1&sca_esv=b60573e6fd3d297f&q=Cat+breeds&uds=AMwkrPu1uc2vS1cm3Oy0-F5NiXmXcccq8FvNnIWEOm5d3VE_ULFU57G_AxTT_J8xznw1j5EI0KrvgREnPTfe9dUqlSsJy-2Xf0h_Gu1TaxVlh0S3g9nCWfc&sa=X&ved=2ahUKEwjnnKCblteEAxX2EFkFHbsaD7cQxKsJegQIFRAB&ictx=0',\n",
       "    'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&num=1&q=Cat+breeds'}],\n",
       "  'header_images': [{'image': 'https://serpapi.com/searches/65e3f179ca968fce70b728f5/images/bc23b677c7dc8305d8ae65387b64153178b54fd3f734b92862aeff3a592e9a3676c9351ded234704.webp',\n",
       "    'source': 'https://www.nationalgeographic.com/animals/mammals/facts/domestic-cat'},\n",
       "   {'image': 'https://serpapi.com/searches/65e3f179ca968fce70b728f5/images/bc23b677c7dc8305d8ae65387b64153178b54fd3f734b928f510d5fd22d79b6963da43a06061ccdf.webp',\n",
       "    'source': 'https://en.wikipedia.org/wiki/Cat'},\n",
       "   {'image': 'https://serpapi.com/searches/65e3f179ca968fce70b728f5/images/bc23b677c7dc8305d8ae65387b64153178b54fd3f734b9285c2f14439c1ca91b32781ffdb45481c1.webp',\n",
       "    'source': 'https://www.wfla.com/bloom-tampa-bay/10-surprising-benefits-of-having-a-cat-in-your-life/'}],\n",
       "  'description': 'The cat, commonly referred to as the domestic cat or house cat, is the only domesticated species in the family Felidae. Recent advances in archaeology and genetics have shown that the domestication of the cat occurred in the Near East around 7500 BC.',\n",
       "  'source': {'name': 'Wikipedia', 'link': 'https://en.wikipedia.org/wiki/Cat'},\n",
       "  'lifespan': '12 – 18 years (Domesticated)',\n",
       "  'scientific_name': 'Felis catus',\n",
       "  'class': 'Mammalia',\n",
       "  'domain': 'Eukaryota',\n",
       "  'family': 'Felidae',\n",
       "  'genus': 'Felis',\n",
       "  'web_results': [{'thumbnail': 'https://serpapi.com/searches/65e3f179ca968fce70b728f5/images/bc23b677c7dc8305d8ae65387b641531116b8b6f9bcb9afcf7b07734fea7d1e87be157f428e5ba52.jpeg',\n",
       "    'title': \"Cat's Meows Are So Misunderstood - The New York Times\",\n",
       "    'snippet': \"A study suggests that humans often misinterpret a pet's signals; even purring doesn't guarantee a contented cat.\",\n",
       "    'source': 'The New York Times',\n",
       "    'source_image': 'https://serpapi.com/searches/65e3f179ca968fce70b728f5/images/bc23b677c7dc8305d8ae65387b641531116b8b6f9bcb9afc8c215c2234105a6d283049de9975d17d0bdc03dde27816e7.png',\n",
       "    'link': 'https://www.nytimes.com/2024/02/29/science/cats-animal-behavior-meow.html',\n",
       "    'date': '2 days ago'},\n",
       "   {'title': 'Lifespan',\n",
       "    'snippet': '12 – 18 years (Domesticated)',\n",
       "    'link': 'https://www.google.com/search?num=1&sca_esv=b60573e6fd3d297f&q=cat+lifespan&stick=H4sIAAAAAAAAAOPgE2LQ0slOttJPyszPyU-v1M8vSk_MyyzOjU_OSSwuzkzLTE4syczPs8rJTEstLkjMW8TKAxRRgHEBwBwQAkEAAAA&sa=X&ved=2ahUKEwjnnKCblteEAxX2EFkFHbsaD7cQ18AJegQIHhAB'},\n",
       "   {'thumbnail': 'https://serpapi.com/searches/65e3f179ca968fce70b728f5/images/bc23b677c7dc8305d8ae65387b641531116b8b6f9bcb9afc690e6d5b412635d7e6e4c0dde819d383.jpeg',\n",
       "    'title': 'Cat Raised By Dogs Races To The Ocean To Swim | The Dodo',\n",
       "    'source': 'YouTube • The Dodo',\n",
       "    'source_image': 'https://serpapi.com/searches/65e3f179ca968fce70b728f5/images/bc23b677c7dc8305d8ae65387b641531116b8b6f9bcb9afca5d131ac2b466a761b32a227b61b4d3559611f9dc3f24ede.png',\n",
       "    'link': 'https://www.youtube.com/watch?v=nVbm5nImTj0',\n",
       "    'duration': '3:03'}],\n",
       "  'breeds': [{'name': 'Siamese cat',\n",
       "    'link': 'https://www.google.com/search?num=1&sca_esv=b60573e6fd3d297f&q=Siamese+cat&stick=H4sIAAAAAAAAAONgFuLQz9U3MKwsqlACs9LjjYu0-P2L0hPzMotzQ_KdilJTUxaxcgdnJuamFqcqJCeW7GBlBADK-9WjOAAAAA&sa=X&ved=2ahUKEwjnnKCblteEAxX2EFkFHbsaD7cQxA16BAgvEAM',\n",
       "    'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&num=1&q=Siamese+cat&stick=H4sIAAAAAAAAAONgFuLQz9U3MKwsqlACs9LjjYu0-P2L0hPzMotzQ_KdilJTUxaxcgdnJuamFqcqJCeW7GBlBADK-9WjOAAAAA',\n",
       "    'image': 'https://serpapi.com/searches/65e3f179ca968fce70b728f5/images/bc23b677c7dc8305d8ae65387b641531c4d92cd751c293ede4338b628e5c45da.jpeg'},\n",
       "   {'name': 'British Shorthair',\n",
       "    'link': 'https://www.google.com/search?num=1&sca_esv=b60573e6fd3d297f&q=British+Shorthair&stick=H4sIAAAAAAAAAONgFuLQz9U3MKwsqlACs5KTjM21-P2L0hPzMotzQ_KdilJTUxaxCjoVZZZkFmcoBGfkF5VkJGYW7WBlBAC1AxqRPgAAAA&sa=X&ved=2ahUKEwjnnKCblteEAxX2EFkFHbsaD7cQxA16BAgvEAU',\n",
       "    'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&num=1&q=British+Shorthair&stick=H4sIAAAAAAAAAONgFuLQz9U3MKwsqlACs5KTjM21-P2L0hPzMotzQ_KdilJTUxaxCjoVZZZkFmcoBGfkF5VkJGYW7WBlBAC1AxqRPgAAAA',\n",
       "    'image': 'https://serpapi.com/searches/65e3f179ca968fce70b728f5/images/bc23b677c7dc8305d8ae65387b641531a700c6300db0194aca9a39be22df9521.jpeg'},\n",
       "   {'name': 'Maine Coon',\n",
       "    'link': 'https://www.google.com/search?num=1&sca_esv=b60573e6fd3d297f&q=Maine+Coon&stick=H4sIAAAAAAAAAONgFuLQz9U3MKwsqlACs7LMDLK0-P2L0hPzMotzQ_KdilJTUxaxcvkmZualKjjn5-ftYGUEAGEJqpQ3AAAA&sa=X&ved=2ahUKEwjnnKCblteEAxX2EFkFHbsaD7cQxA16BAgvEAc',\n",
       "    'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&num=1&q=Maine+Coon&stick=H4sIAAAAAAAAAONgFuLQz9U3MKwsqlACs7LMDLK0-P2L0hPzMotzQ_KdilJTUxaxcvkmZualKjjn5-ftYGUEAGEJqpQ3AAAA',\n",
       "    'image': 'https://serpapi.com/searches/65e3f179ca968fce70b728f5/images/bc23b677c7dc8305d8ae65387b641531a26b0212e27cf0215bb424cdbff9effd.jpeg'},\n",
       "   {'name': 'Persian cat',\n",
       "    'link': 'https://www.google.com/search?num=1&sca_esv=b60573e6fd3d297f&q=Persian+cat&stick=H4sIAAAAAAAAAONgFuLQz9U3MKwsqlACs7JMzMq0-P2L0hPzMotzQ_KdilJTUxaxcgekFhVnJuYpJCeW7GBlBABKW7RSOAAAAA&sa=X&ved=2ahUKEwjnnKCblteEAxX2EFkFHbsaD7cQxA16BAgvEAk',\n",
       "    'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&num=1&q=Persian+cat&stick=H4sIAAAAAAAAAONgFuLQz9U3MKwsqlACs7JMzMq0-P2L0hPzMotzQ_KdilJTUxaxcgekFhVnJuYpJCeW7GBlBABKW7RSOAAAAA',\n",
       "    'image': 'https://serpapi.com/searches/65e3f179ca968fce70b728f5/images/bc23b677c7dc8305d8ae65387b641531ad9ac94072e5f2ac8d91de789130dd78.jpeg'}]},\n",
       " 'organic_results': [{'position': 1,\n",
       "   'title': 'Cat',\n",
       "   'link': 'https://en.wikipedia.org/wiki/Cat',\n",
       "   'redirect_link': 'https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://en.wikipedia.org/wiki/Cat&ved=2ahUKEwjnnKCblteEAxX2EFkFHbsaD7cQFnoECCEQAQ',\n",
       "   'displayed_link': 'https://en.wikipedia.org › wiki › Cat',\n",
       "   'thumbnail': 'https://serpapi.com/searches/65e3f179ca968fce70b728f5/images/6a628cddabd945593000d23f531295a1150dc7d07f3d4267563ab7dc2f131346.jpeg',\n",
       "   'favicon': 'https://serpapi.com/searches/65e3f179ca968fce70b728f5/images/6a628cddabd945593000d23f531295a11c27ecb5e60564c47ffc4ead682ad16b.png',\n",
       "   'snippet': 'The cat (Felis catus), commonly referred to as the domestic cat or house cat, is the only domesticated species in the family Felidae.',\n",
       "   'snippet_highlighted_words': ['cat', 'cat', 'cat'],\n",
       "   'sitelinks': {'inline': [{'title': 'List of cat breeds',\n",
       "      'link': 'https://en.wikipedia.org/wiki/List_of_cat_breeds'},\n",
       "     {'title': 'Cat behavior',\n",
       "      'link': 'https://en.wikipedia.org/wiki/Cat_behavior'},\n",
       "     {'title': 'Cat anatomy',\n",
       "      'link': 'https://en.wikipedia.org/wiki/Cat_anatomy'},\n",
       "     {'title': 'Feral cat',\n",
       "      'link': 'https://en.wikipedia.org/wiki/Feral_cat'}]},\n",
       "   'source': 'Wikipedia'}],\n",
       " 'related_searches': [{'block_position': 1,\n",
       "   'query': 'cat or dog',\n",
       "   'link': 'https://www.google.com/search?num=1&sca_esv=b60573e6fd3d297f&q=Cat+or+dog&sa=X&ved=2ahUKEwjnnKCblteEAxX2EFkFHbsaD7cQ1QJ6BAgSEAE',\n",
       "   'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&num=1&q=Cat+or+dog'},\n",
       "  {'block_position': 1,\n",
       "   'query': 'cat drawing',\n",
       "   'link': 'https://www.google.com/search?num=1&sca_esv=b60573e6fd3d297f&q=Cat+drawing&sa=X&ved=2ahUKEwjnnKCblteEAxX2EFkFHbsaD7cQ1QJ6BAgTEAE',\n",
       "   'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&num=1&q=Cat+drawing'},\n",
       "  {'block_position': 1,\n",
       "   'query': 'cat game',\n",
       "   'link': 'https://www.google.com/search?num=1&sca_esv=b60573e6fd3d297f&q=Cat+game&sa=X&ved=2ahUKEwjnnKCblteEAxX2EFkFHbsaD7cQ1QJ6BAgREAE',\n",
       "   'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&num=1&q=Cat+game'},\n",
       "  {'block_position': 1,\n",
       "   'query': 'cat sound',\n",
       "   'link': 'https://www.google.com/search?num=1&sca_esv=b60573e6fd3d297f&q=Cat+sound&sa=X&ved=2ahUKEwjnnKCblteEAxX2EFkFHbsaD7cQ1QJ6BAgQEAE',\n",
       "   'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&num=1&q=Cat+sound'},\n",
       "  {'block_position': 1,\n",
       "   'query': 'cute cat',\n",
       "   'link': 'https://www.google.com/search?num=1&sca_esv=b60573e6fd3d297f&q=Cute+cat&sa=X&ved=2ahUKEwjnnKCblteEAxX2EFkFHbsaD7cQ1QJ6BAgPEAE',\n",
       "   'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&num=1&q=Cute+cat'},\n",
       "  {'block_position': 1,\n",
       "   'query': 'cat meme',\n",
       "   'link': 'https://www.google.com/search?num=1&sca_esv=b60573e6fd3d297f&q=Cat+meme&sa=X&ved=2ahUKEwjnnKCblteEAxX2EFkFHbsaD7cQ1QJ6BAgOEAE',\n",
       "   'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&num=1&q=Cat+meme'},\n",
       "  {'block_position': 1,\n",
       "   'query': 'cats for sale',\n",
       "   'link': 'https://www.google.com/search?num=1&sca_esv=b60573e6fd3d297f&q=Cats+for+sale&sa=X&ved=2ahUKEwjnnKCblteEAxX2EFkFHbsaD7cQ1QJ6BAgNEAE',\n",
       "   'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&num=1&q=Cats+for+sale'},\n",
       "  {'block_position': 1,\n",
       "   'query': 'cat full form',\n",
       "   'link': 'https://www.google.com/search?num=1&sca_esv=b60573e6fd3d297f&q=CAT+Full+Form&sa=X&ved=2ahUKEwjnnKCblteEAxX2EFkFHbsaD7cQ1QJ6BAgMEAE',\n",
       "   'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&num=1&q=CAT+Full+Form'}],\n",
       " 'pagination': {'current': 1,\n",
       "  'next': 'https://www.google.com/search?q=cat&oq=cat&num=1&start=1&sourceid=chrome&ie=UTF-8',\n",
       "  'other_pages': {'2': 'https://www.google.com/search?q=cat&oq=cat&num=1&start=1&sourceid=chrome&ie=UTF-8',\n",
       "   '3': 'https://www.google.com/search?q=cat&oq=cat&num=1&start=2&sourceid=chrome&ie=UTF-8',\n",
       "   '4': 'https://www.google.com/search?q=cat&oq=cat&num=1&start=3&sourceid=chrome&ie=UTF-8',\n",
       "   '5': 'https://www.google.com/search?q=cat&oq=cat&num=1&start=4&sourceid=chrome&ie=UTF-8'}},\n",
       " 'serpapi_pagination': {'current': 1,\n",
       "  'next_link': 'https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&num=1&q=cat&start=1',\n",
       "  'next': 'https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&num=1&q=cat&start=1',\n",
       "  'other_pages': {'2': 'https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&num=1&q=cat&start=1',\n",
       "   '3': 'https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&num=1&q=cat&start=2',\n",
       "   '4': 'https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&num=1&q=cat&start=3',\n",
       "   '5': 'https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&num=1&q=cat&start=4'}}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"q\": \"cat\",\n",
    "    \"api_key\": 'd7555dcb6698cf4004b757eeade1ac86a425b101cb972691dc757282f353d9f6',\n",
    "    \"num\": 1\n",
    "}\n",
    "search = GoogleSearch(params)\n",
    "search.get_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'news_results'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m search \u001b[38;5;241m=\u001b[39m GoogleSearch(params)\n\u001b[1;32m     15\u001b[0m results \u001b[38;5;241m=\u001b[39m search\u001b[38;5;241m.\u001b[39mget_dict()\n\u001b[0;32m---> 16\u001b[0m news_results \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnews_results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     17\u001b[0m news_results\n",
      "\u001b[0;31mKeyError\u001b[0m: 'news_results'"
     ]
    }
   ],
   "source": [
    "from serpapi import GoogleSearch\n",
    "\n",
    "from_date = '2019-02-22'\n",
    "to_date = '2024-03-03'\n",
    "\n",
    "params = {\n",
    "  \"q\": 'Ambani Anant Ambani marriage site:http://indiatoday.in OR site:http://timesofindia.indiatimes.com',\n",
    "  \"tbm\": \"nws\",\n",
    "  \"api_key\": \"d7555dcb6698cf4004b757eeade1ac86a425b101cb972691dc757282f353d9f6\",\n",
    "  \"tbs\": f\"cdr:1,cd_min:{from_date.replace('-', '/')},cd_max:{to_date.replace('-', '/')}\",\n",
    "  'num' : 100\n",
    "}\n",
    "\n",
    "search = GoogleSearch(params)\n",
    "results = search.get_dict()\n",
    "news_results = results[\"news_results\"]\n",
    "news_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'search_metadata': {'id': '65e3fcbf477c0e45c476fabf',\n",
       "  'status': 'Success',\n",
       "  'json_endpoint': 'https://serpapi.com/searches/1aaeaac6d67b6acc/65e3fcbf477c0e45c476fabf.json',\n",
       "  'created_at': '2024-03-03 04:29:51 UTC',\n",
       "  'processed_at': '2024-03-03 04:29:51 UTC',\n",
       "  'google_url': 'https://www.google.com/search?q=Ambani+Anant+Ambani+marriage+site%3Ahttp%3A%2F%2Findiatoday.in+OR+site%3Ahttp%3A%2F%2Ftimesofindia.indiatimes.com&oq=Ambani+Anant+Ambani+marriage+site%3Ahttp%3A%2F%2Findiatoday.in+OR+site%3Ahttp%3A%2F%2Ftimesofindia.indiatimes.com&num=100&tbm=nws&tbs=cdr:1,cd_min:2019/02/22,cd_max:2024/03/03&sourceid=chrome&ie=UTF-8',\n",
       "  'raw_html_file': 'https://serpapi.com/searches/1aaeaac6d67b6acc/65e3fcbf477c0e45c476fabf.html',\n",
       "  'total_time_taken': 0.83},\n",
       " 'search_parameters': {'engine': 'google',\n",
       "  'q': 'Ambani Anant Ambani marriage site:http://indiatoday.in OR site:http://timesofindia.indiatimes.com',\n",
       "  'google_domain': 'google.com',\n",
       "  'num': '100',\n",
       "  'device': 'desktop',\n",
       "  'tbm': 'nws',\n",
       "  'tbs': 'cdr:1,cd_min:2019/02/22,cd_max:2024/03/03'},\n",
       " 'search_information': {'query_displayed': 'Ambani Anant Ambani marriage site:http://indiatoday.in OR site:http://timesofindia.indiatimes.com',\n",
       "  'news_results_state': 'Fully empty'},\n",
       " 'error': \"Google hasn't returned any results for this query.\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "\n",
    "def send_email(sender_email, sender_password, receiver_email, subject, message):\n",
    "    # Set up the MIME\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = sender_email\n",
    "    msg['To'] = receiver_email\n",
    "    msg['Subject'] = subject\n",
    "\n",
    "    # Attach the message to the email\n",
    "    msg.attach(MIMEText(message, 'plain'))\n",
    "\n",
    "    # Create SMTP session\n",
    "    server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    server.starttls()\n",
    "\n",
    "    # Login to the SMTP server\n",
    "    server.login(sender_email, sender_password)\n",
    "\n",
    "    # Send email\n",
    "    server.sendmail(sender_email, receiver_email, msg.as_string())\n",
    "\n",
    "    # Terminate the SMTP session\n",
    "    server.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_email = \"trolldude326@gmail.com\"\n",
    "sender_password = \"mafazu03\"\n",
    "\n",
    "receiver_email = \"petraoil.prod@gmail.com\"\n",
    "subject = \"test\"\n",
    "message = \"Hello world!\"\n",
    "\n",
    "send_email(sender_email, sender_password, receiver_email, subject, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "import ssl\n",
    "import certifi\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.image import MIMEImage\n",
    "\n",
    "port = 587  # For starttls\n",
    "smtp_server = \"smtp.gmail.com\"\n",
    "sender_email = \"bat463660@gmail.com\"\n",
    "receiver_email = \"petraoil.prod@gmail.com\"\n",
    "password = \"bygc aape tnem adev\"\n",
    "\n",
    "# Create a multipart message\n",
    "message = MIMEMultipart()\n",
    "message[\"From\"] = sender_email\n",
    "message[\"To\"] = receiver_email\n",
    "message[\"Subject\"] = \"Subject: Hello\"\n",
    "\n",
    "# Add body to email\n",
    "message.attach(MIMEText(\"wow\", \"plain\"))\n",
    "\n",
    "# Open the image file and attach it to the email\n",
    "with open(\"dashboard/dashboard_plot.png\", \"rb\") as attachment:\n",
    "    image_part = MIMEImage(attachment.read(), name=\"dashbaord.jpg\")\n",
    "\n",
    "# Add image to the email\n",
    "message.attach(image_part)\n",
    "\n",
    "context = ssl.create_default_context(cafile=certifi.where())\n",
    "\n",
    "with smtplib.SMTP(smtp_server, port) as server:\n",
    "    server.starttls(context=context)\n",
    "    server.login(sender_email, password)\n",
    "    server.sendmail(sender_email, receiver_email, message.as_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi: 0'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HEL = 0\n",
    "f\"Hi: {HEL}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/Users/mohamedmafaz/Downloads/50-contacts.csv\")\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'=   first_name  last_name                   company_name                address             city                county state    zip        phone1         phone                          email\\n0       James       Butt              Benton, John B Jr     6649 N Blue Gum St      New Orleans               Orleans    LA  70116  504-621-8927  504-845-1427                jbutt@gmail.com\\n1   Josephine    Darakjy          Chanay, Jeffrey A Esq    4 B Blue Ridge Blvd         Brighton            Livingston    MI  48116  810-292-9388  810-374-9840  josephine_darakjy@darakjy.org\\n2         Art     Venere            Chemel, James L Cpa   8 W Cerritos Ave #54       Bridgeport            Gloucester    NJ   8014  856-636-8749  856-264-4130                 art@venere.org\\n3       Lenna   Paprocki         Feltz Printing Service            639 Main St        Anchorage             Anchorage    AK  99501  907-385-4412  907-921-2010          lpaprocki@hotmail.com\\n4     Donette     Foller            Printing Dimensions           34 Center St         Hamilton                Butler    OH  45011  513-570-1893  513-549-4561         donette.foller@cox.net\\n5      Simona    Morasca            Chapman, Ross E Esq           3 Mcauley Dr          Ashland               Ashland    OH  44805  419-503-2484  419-800-6759             simona@morasca.com\\n6      Mitsue    Tollner             Morlong Associates              7 Eads St          Chicago                  Cook    IL  60632  773-573-6914  773-924-8565       mitsue_tollner@yahoo.com\\n7       Leota   Dilliard               Commercial Press       7 W Jackson Blvd         San Jose           Santa Clara    CA  95111  408-752-3500  408-813-1105              leota@hotmail.com\\n8        Sage     Wieser      Truhlar And Truhlar Attys       5 Boston Ave #88      Sioux Falls             Minnehaha    SD  57105  605-414-2147  605-794-4895            sage_wieser@cox.net\\n9        Kris    Marrier        King, Christopher A Esq  228 Runamuck Pl #2808        Baltimore        Baltimore City    MD  21224  410-655-8723  410-804-4694                 kris@gmail.com\\n10      Minna     Amigon              Dorl, James J Esq       2371 Jerrold Ave       Kulpsville            Montgomery    PA  19443  215-874-1229  215-422-8694         minna_amigon@yahoo.com\\n11       Abel    Maclead            Rangoni Of Florence     37275 St  Rt 17m M    Middle Island               Suffolk    NY  11953  631-335-3414  631-677-3675             amaclead@gmail.com\\n12      Kiley  Caldarera                    Feiner Bros       25 E 75th St #69      Los Angeles           Los Angeles    CA  90034  310-498-5651  310-254-3084        kiley.caldarera@aol.com\\n13   Graciela       Ruta        Buckley Miller & Wright  98 Connecticut Ave Nw    Chagrin Falls                Geauga    OH  44023  440-780-8425  440-579-7763                  gruta@cox.net\\n14      Cammy    Albares         Rousseaux, Michael Esq       56 E Morehead St           Laredo                  Webb    TX  78045  956-537-6195  956-841-7216             calbares@gmail.com\\n15     Mattie   Poquette         Century Communications    73 State Road 434 E          Phoenix              Maricopa    AZ  85013  602-277-4385  602-953-6360                 mattie@aol.com\\n16    Meaghan     Garufi             Bolton, Wilbur Esq    69734 E Carrillo St     Mc Minnville                Warren    TN  37110  931-313-9635  931-235-7959            meaghan@hotmail.com\\n17     Gladys        Rim          T M Byxbee Company Pc   322 New Horizon Blvd        Milwaukee             Milwaukee    WI  53207  414-661-9598  414-377-2880             gladys.rim@rim.org\\n18       Yuki    Whobrey        Farmers Insurance Group       1 State Route 27           Taylor                 Wayne    MI  48180  313-288-7937  313-341-4470           yuki_whobrey@aol.com\\n19   Fletcher      Flosi         Post Box Services Plus    394 Manchester Blvd         Rockford             Winnebago    IL  61109  815-828-2147  815-426-5657       fletcher.flosi@yahoo.com\\n20      Bette      Nicka                   Sport En Art            6 S 33rd St            Aston              Delaware    PA  19014  610-545-3615  610-492-4643            bette_nicka@cox.net\\n21   Veronika     Inouye                C 4 Network Inc        6 Greenleaf Ave         San Jose           Santa Clara    CA  95111  408-540-1785  408-813-4592                vinouye@aol.com\\n22    Willard    Kolmetz          Ingalls, Donald R Esq       618 W Yakima Ave           Irving                Dallas    TX  75062  972-303-9197  972-896-4882            willard@hotmail.com\\n23    Maryann    Royster          Franklin, Peter L Esq       74 S Westgate St           Albany                Albany    NY  12204  518-966-7987  518-448-8982           mroyster@royster.com\\n24     Alisha  Slusarski              Wtlz Power 107 Fm          3273 State St        Middlesex             Middlesex    NJ   8846  732-658-3154  732-635-3453           alisha@slusarski.com\\n25     Allene   Iturbide             Ledecky, David Esq          1 Central Ave    Stevens Point               Portage    WI  54481  715-662-6764  715-530-9863        allene_iturbide@cox.net\\n26     Chanel      Caudy         Professional Image Inc    86 Nw 66th St #8673          Shawnee               Johnson    KS  66218  913-388-2079  913-899-1103         chanel.caudy@caudy.org\\n27    Ezekiel       Chui            Sider, Donald C Esq        2 Cedar Ave #84           Easton                Talbot    MD  21601  410-669-1642  410-235-8738               ezekiel@chui.com\\n28     Willow      Kusko                      U Pull It     90991 Thorburn Ave         New York              New York    NY  10011  212-582-4976  212-934-5167               wkusko@yahoo.com\\n29   Bernardo    Figeroa             Clark, Richard Cpa          386 9th Ave N           Conroe            Montgomery    TX  77301  936-336-3951  936-597-3614               bfigeroa@aol.com\\n30      Ammie     Corrio             Moskowitz, Barry S     74874 Atlantic Ave         Columbus              Franklin    OH  43215  614-801-9788  614-648-3265               ammie@corrio.com\\n31   Francine    Vocelka    Cascade Realty Advisors Inc           366 South Dr       Las Cruces              Dona Ana    NM  88011  505-977-3911  505-335-5293   francine_vocelka@vocelka.com\\n32      Ernie   Stenseth                 Knwz Newsradio        45 E Liberty St  Ridgefield Park                Bergen    NJ   7660  201-709-6245  201-387-9093         ernie_stenseth@aol.com\\n33     Albina      Glick           Giampetro, Anthony D             4 Ralph Ct         Dunellen             Middlesex    NJ   8812  732-924-7882  732-782-6701               albina@glick.com\\n34    Alishia      Sergi        Milford Enterprises Inc  2742 Distribution Way         New York              New York    NY  10025  212-860-1579  212-753-2740               asergi@gmail.com\\n35    Solange     Shinko              Mosocco, Ronald A            426 Wolf St         Metairie             Jefferson    LA  70002  504-979-9175  504-265-8174             solange@shinko.com\\n36       Jose   Stockham          Tri State Refueler Co        128 Bransten Rd         New York              New York    NY  10011  212-675-8570  212-569-4233                 jose@yahoo.com\\n37    Rozella   Ostrosky                Parkway Company         17 Morena Blvd        Camarillo               Ventura    CA  93012  805-832-6163  805-609-1531  rozella.ostrosky@ostrosky.com\\n38  Valentine    Gillian           Fbs Business Finance          775 W 17th St      San Antonio                 Bexar    TX  78204  210-812-9597  210-300-6244    valentine_gillian@gmail.com\\n39       Kati  Rulapaugh  Eder Assocs Consltng Engrs Pc        6980 Dorsett Rd          Abilene             Dickinson    KS  67410  785-463-7829  785-219-7724     kati.rulapaugh@hotmail.com\\n40   Youlanda   Schemmer                 Tri M Tool Inc          2881 Lewis Rd       Prineville                 Crook    OR  97754  541-548-8197  541-993-2611               youlanda@aol.com\\n41       Dyan    Oldroyd      International Eyelets Inc      7219 Woodfield Rd    Overland Park               Johnson    KS  66204  913-413-4604  913-645-8918               doldroyd@aol.com\\n42     Roxane    Campain             Rapid Trading Intl           1048 Main St        Fairbanks  Fairbanks North Star    AK  99708  907-231-4722  907-335-6568             roxane@hotmail.com\\n43     Lavera      Perin            Abc Enterprises Inc            678 3rd Ave            Miami            Miami-Dade    FL  33196  305-606-7291  305-995-2078               lperin@perin.org\\n44      Erick    Ferencz        Cindy Turner Associates        20 S Babcock St        Fairbanks  Fairbanks North Star    AK  99712  907-741-1044  907-227-6777          erick.ferencz@aol.com\\n45     Fatima    Saylors           Stanton, James D Esq       2 Lighthouse Ave          Hopkins              Hennepin    MN  55343  952-768-2416  952-479-2375           fsaylors@saylors.org\\n46       Jina   Briddick             Grace Pastries Inc        38938 Park Blvd           Boston               Suffolk    MA   2128  617-399-5124  617-997-5771     jina_briddick@briddick.com\\n47    Kanisha    Waycott            Schroer, Gene E Esq          5 Tomahawk Dr      Los Angeles           Los Angeles    CA  90006  323-453-2780  323-315-7314      kanisha_waycott@yahoo.com\\n48    Emerson     Bowley                    Knights Inn          762 S Main St          Madison                  Dane    WI  53711  608-336-7444  608-658-7940      emerson.bowley@bowley.org\\n49      Blair      Malet  Bollinger Mach Shp & Shipyard          209 Decker Dr     Philadelphia          Philadelphia    PA  19132  215-907-9111  215-794-4519               bmalet@yahoo.com'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = ''\n",
    "t+=\"=\"+df.to_string()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q python-docx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(\"/Users/mohamedmafaz/Downloads/Unit 1.docx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for paragraph in doc.paragraphs:\n",
    "    text += paragraph.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Introduction to Natural Language ProcessingLearning ObjectivesBy the end of this chapter, you will be able to:Describe what natural language processing (NLP) is all aboutDescribe the history of NLPDifferentiate between NLP and Text AnalyticsImplement various preprocessing tasksDescribe the various phases of an NLP projectIn this chapter, you will learn about the basics of natural language processing and various preprocessing steps that are required to clean and analyze the data.\\xa0 IntroductionTo start with looking at NLP, let\\'s understand what natural language is. In simple terms, it\\'s the language we use to express ourselves. It\\'s a basic means of communication. To define more specifically, language is a mutually agreed set of protocols involving words/sounds we use to communicate with each other.In this era of digitization and computation, we tend to comprehend language scientifically. This is because we are constantly trying to make inanimate objects understand us. Thus, it has become essential to develop mechanisms by which language can be fed to inanimate objects such as computers. NLP helps us do this.Let\\'s look at an example. You must have some emails in your mailbox that have been automatically labeled as spam. This is done with the help of NLP. Here, an inanimate object – the email service – analyzes the content of the emails, comprehends it, and then further decides whether these emails need to be marked as spam or not.\\xa0 History of NLPNLP is an area that overlaps with others. It has emerged from fields such as artificial intelligence, linguistics, formal languages, and compilers. With the advancement of computing technologies and the increased availability of data, the way natural language is being processed has changed. Previously, a traditional rule-based system was used for computations. Today, computations on natural language are being done using machine learning and deep learning techniques.The major work on machine learning-based NLP started during the 1980s. During the 1980s, developments across various disciplines such as artificial intelligence, linguistics, formal languages, and computations led to the emergence of an interdisciplinary subject called NLP. In the next section, we\\'ll look at text analytics and how it differs from NLP.\\xa0 Text Analytics and NLPText analytics is the method of extracting meaningful insights and answering questions from text data. This text data need not be a human language. Let\\'s understand this with an example. Suppose you have a text file that contains your outgoing phone calls and SMS log data in the following format:Figure 1.1: Format of call dataIn the preceding figure, the first two fields represent the date and time at which the call was made or the SMS was sent. The third field represents the type of data. If the data is of the call type, then the value for this field will be set as voice_call. If the type of data is sms, the value of this field will be set to sms. The fourth field is for the phone number and name of the contact. If the number of the person is not in the contact list, then the name value will be left blank. The last field is for the duration of the call or text message. If the type of the data is voice_call, then the value in this field will be the duration of that call. If the type of data is sms, then the value in this field will be the text message. The following figure shows records of call data stored in a text file:Figure 1.2: Call records in a text fileNow, the data shown in the preceding figure is not exactly a human language. But it contains various information that can be extracted by analyzing it. A couple of questions that can be answered by looking at this data are as follows:How many New Year greetings were sent by SMS on 1st January? How many people were contacted whose name is not in the contact list? The art of extracting useful insights from any given text data can be referred to as text analytics. NLP, on the other hand, is not just restricted to text data. Voice (speech) recognition and analysis also come under the domain of NLP. NLP can be broadly categorized into two types: Natural Language Understanding (NLU) and Natural Language Generation (NLG). A proper explanation of these terms is provided as follows:NLU: NLU refers to a process by which an inanimate object with computing power is able to comprehend spoken language.NLG: NLG refers to a process by which an inanimate object with computing power is able to manifest its thoughts in a language that humans are able to understand.For example, when a human speaks to a machine, the machine interprets the human language with the help of the NLU process. Also, by using the NLG process, the machine generates an appropriate response and shares that with the human, thus making it easier for humans to understand. These tasks, which are part of NLP, are not part of text analytics. Now we will look at an exercise that will give us a better understanding of text analytics.Exercise 1: Basic Text AnalyticsIn this exercise, we will perform some basic text analytics on the given text data. Follow these steps to implement this exercise:Open a Jupyter notebook.Insert a new cell. Assign a sentence variable with \\'The quick brown fox jumps over the lazy dog\\'. Insert a new cell and add the following code to implement this:sentence = \\'The quick brown fox jumps over the lazy dog\\'Check whether the word \\'quick\\' belongs to that text using the following code:\\'quick\\' in sentence\\uf0b7  The preceding code will return the output \\'True\\'.\\uf0b7  Find out the index value of the word \\'fox\\' using the following code:sentence.index(\\'fox\\')\\uf0b7  The code will return the output 16.\\uf0b7  To find out the rank of the word \\'lazy\\', use the following code:sentence.split().index(\\'lazy\\')\\uf0b7  The code generates the output 7.\\uf0b7  For printing the third word of the given text, use the following code:sentence.split()[2]\\uf0b7  This will return the output \\'brown\\'.\\uf0b7  To print the third word of the given sentence in reverse order, use the following code:sentence.split()[2][::-1]\\uf0b7  This will return the output \\'nworb\\'.\\uf0b7  To concatenate the first and last words of the given sentence, use the following code:words = sentence.split()first_word = words[0]last_word = words[len(words)-1]concat_word = first_word + last_word print(concat_word)\\uf0b7  The code will generate the output \\'Thedog\\'.\\uf0b7  For printing words at even positions, use the following code:[words[i] for i in range(len(words)) if i%2 == 0]\\uf0b7  The code generates the following output:Figure 1.3: List of words at even positions\\uf0b7  To print the last three letters of the text, use the following code:sentence[-3:]\\uf0b7  This will generate the output \\'dog\\'.\\uf0b7  To print the text in reverse order, use the following code:sentence[::-1]\\uf0b7  The code generates the following output:Figure 1.4: Text in reverse order\\uf0b7  To print each word of the given text in reverse order, maintaining their sequence, use the following code:print(\\' \\'.join([word[::-1] for word in words]))The code generates the following output:Figure 1.5: Printing the text in reverse order while preserving word sequenceWe are now well acquainted with NLP. In the next section, let\\'s dive deeper into the various steps involved in it.\\xa0 Various Steps in NLPEarlier, we talked about the types of computations that are done on natural language. There are various standard NLP tasks. Apart from these tasks, you have the ability to design your own tasks as per your requirements. In the coming sections, we will be discussing various preprocessing tasks in detail and demonstrating them with an exercise.TokenizationTokenization refers to the procedure of splitting a sentence into its constituent words. For example, consider this sentence: \"I am reading a book.\" Here, our task is to extract words/tokens from this sentence. After passing this sentence to a tokenization program, the extracted words/tokens would be \"I\", \"am\", \"reading\", \"a\", \"book\", and \".\". This example extracts one token at a time. Such tokens are called unigrams. However, we can also extract two or three tokens at a time. We need to extract tokens because, for the sake of convenience, we tend to analyze natural language word by word. If we extract two tokens at a time, it is called bigrams. If three tokens, it is called trigrams. Based on the requirements, n-grams can be extracted (where \"n\" is a natural number). Noten-gram refers to a sequence of n items from a given text.Let\\'s now try extracting trigrams from the following sentence: \"The sky is blue.\" Here, the first trigram would be \"The sky is\". The second would be \"sky is blue\". This might sound easy. However, tokenization can be difficult at times. For instance, consider this sentence: \"I would love to visit the United States\". The tokens generated are \"I\", \"would\", \"love\", \"to\", \"visit\", \"the\", and \"United States\". Here, \"United States\" has to be treated as a single entity. Individual words such as \"United\" and \"States\" do not make any sense here.To get a better understanding of tokenization, let\\'s solve an exercise based on it in the next section.Exercise 2: Tokenization of a Simple SentenceIn this exercise, we will tokenize the words in a given sentence with the help of the NLTK library. Follow these steps to implement this exercise:Open a Jupyter notebook.Insert a new cell and add a following code to import the necessary libraries:import nltkfrom nltk import word_tokenize\\uf0b7  The word_tokenize() method is used to split the sentence into words/tokens. We need to add a sentence as input to the word_tokenize() method, so that it performs its job. The result obtained would be a list, which we will store in a word variable. To implement this, insert a new cell and add the following code:words = word_tokenize(\"I am reading NLP Fundamentals\")In order to view the list of tokens generated, we need to view it using the print() function. Insert a new cell and add the following code to implement this:print(words)The code generates the following output:Figure 1.6: List of tokensThus we can see the list of tokens generated with the help of the word_tokenize() method. In the next section, we will see another pre-processing step: Parts-of-Speech (PoS) tagging.PoS TaggingPoS refers to parts of speech. PoS tagging refers to the process of tagging words within sentences into their respective parts of speech and then finally labeling them. We extract Part of Speech of tokens constituting a sentence, so that we can filter out the PoS that are of interest and analyze them. For example, if we look at the sentence, \"The sky is blue,\" we get four tokens – \"The,\" \"sky,\" \"is,\" and \"blue\" – with the help of tokenization. Now, using PoS tagger, we tag parts of speech to each word/token. This will look as follows:[(\\'The\\', \\'DT\\'), (\\'sky\\', \\'NN\\'), (\\'is\\', \\'VBZ\\'), (\\'blue\\', \\'JJ\\')]DT = determinerNN = noun, common, singular or massVBZ = verb, present tense, 3rd person singularJJ = AdjectiveAn exercise in the next section will definitely give a better understanding of this concept.Exercise 3: PoS TaggingIn this exercise, we will find out the PoS for each word in the sentence, \"I am reading NLP Fundamentals\". We first make use of tokenization in order to get the tokens. Later, we use a PoS tagger, which will help us find PoS for each word/token. Follow these steps to implement this exercise:Open a Jupyter notebook.Insert a new cell and add the following code to import the necessary libraries:import nltkfrom nltk import word_tokenize\\uf0b7  For finding the tokens in the sentence, we make use of the word_tokenize() method. Insert a new cell and add the following code to implement this:words = word_tokenize(\"I am reading NLP Fundamentals\")Print the tokens with the help of the print() function. To implement this, add a new cell and write the following code:print(words)\\uf0b7  The code generates the following output:Figure 1.7: List of tokens\\uf0b7  In order to find the PoS for each word, we make use of the pos_tag() method of the nltk library. Insert a new cell and add the following code to implement this:nltk.pos_tag(words)The code generates the following output:Figure 1.8: PoS tag of wordsIn the preceding output, we can see that for each token, a PoS has been allotted. Here, PRP stands for personal pronoun, VBP stands for verb present, VGB stands for verb gerund, NNP stands for proper noun singular, and NNS stands for noun plural.We have learned about labeling appropriate PoS to tokens in a sentence. In the next section, we will learn about stop words in sentences and ways to deal with them.Stop Word RemovalStop words are common words that are just used to support the construction of sentences. We remove stop words from our analysis as they do not impact the meaning of sentences they are present in. Examples of stop words include a, am, and the. Since they occur very frequently and their presence doesn\\'t have much impact on the sense of the sentence, they need to be removed. In the next section, we will look at the practical implementation of removing stop words from a given sentence.Exercise 4: Stop Word RemovalIn this exercise, we will check the list of stopwords provided by the nltk library. Based on this list, we will filter out the stopwords included in our text. Follow these steps to implement this exercise:Open a Jupyter notebook.Insert a new cell and add the following code to import the necessary libraries:import nltknltk.download(\\'stopwords\\')from nltk import word_tokenizefrom nltk.corpus import stopwords\\uf0b7  In order to check the list of stopwords provided for the English language, we pass it as a parameter to the words() function. Insert a new cell and add the following code to implement this:stop_words = stopwords.words(\\'English\\')In the code, the list of stopwords provided by the English language is stored in the stop_words variable. In order to view the list, we make use of the print() function. Insert a new cell and add the following code to view the list:print(stop_words)\\uf0b7  The code generates the following output:Figure 1.9: List of stopwords provided by the English language\\uf0b7  To remove the stop words from a sentence, we first assign a string to the sentence variable and tokenize it into words using the word_tokenize() method. Insert a new cell and add the following code to implement this: sentence = \"I am learning Python. It is one of the most popular programming languages\"sentence_words = word_tokenize(sentence)To print the list of tokens, insert a new cell and add the following code:print(sentence_words)\\uf0b7  The code generates the following output:Figure 1.10: List of tokens in the sentence_words variable\\uf0b7  To remove the stopwords, first we need to loop through each word in the sentence, check whether there are any stop words, and then finally combine them to form a complete sentence. To implement this, insert a new cell and add the following code:sentence_no_stops = \\' \\'.join([word for word in sentence_words if word not in stop_words])To check whether the stopwords are filtered out from our sentence, we print the sentence_no_stops variable. Insert a new cell and add the following code to print:print(sentence_no_stops)The code generates the following output:Figure 1.11: Text without stopwordsAs you can see in the preceding figure, stopwords such as \"am,\" \"is,\" \"of,\" \"the,\" and \"most\" are being filtered out and text without stop words is produced as output.We have learned how to remove stop words from given text. In the next section, we will focus on normalizing text.Text NormalizationThere are some words that are spelt, pronounced, and represented differently, for example, words such as Mumbai and Bombay, and US and United States. Although they are different, they mean the same thing. There are also different forms words that need to be converted into base forms. For example, words such as \"does\" and \"doing,\" when converted to their base form, become \"do\". Along these lines, text normalization is a process wherein different variations of text get converted into a standard form. We need to perform text normalization as there are some words that can mean the same thing as each other. There are various ways of normalizing text, such as spelling correction, stemming, and lemmatization, which will be covered later.For a better understanding of this topic, we will look into practical implementation in the next section.Exercise 5: Text NormalizationIn this exercise, we will normalize a given text. Basically, we will be trying to replace selected words with new words, using the replace() function, and finally produce the normalized text. Follow these steps to implement this exercise:Open a Jupyter notebook.Insert a new cell and add the following code to assign a string to the sentence variable:sentence = \"I visited US from UK on 22-10-18\"\\uf0b7  We want to replace \"US\" with \"United States\", \"UK\" with \"United Kingdom\", and \"18\" with \"2018\". To do so, we make use of the replace() function and store the updated output in the \"normalized_sentence\" variable. Insert a new cell and add the following code to implement this:normalized_sentence = sentence.replace(\"US\", \"United States\").replace(\"UK\", \"United Kingdom\").replace(\"-18\", \"-2018\")Now, in order to check whether the text has been normalized, we insert a new cell and add the following code to print it:print(normalized_sentence)The code generates the following output:Figure 1.12: Normalized textIn the preceding figure, we can see that our text has been normalized.Now that we have learned the basics of text normalization, in the next section, we explore various other ways that text can be normalized.Spelling CorrectionSpelling correction is one of the most important tasks in any NLP project. It can be time consuming, but without it there are high chances of losing out on required information. We make use of the \"autocorrect\" Python library to correct spellings. Let\\'s look at the following exercise to get a better understanding of this.Exercise 6: Spelling Correction of a Word and a SentenceIn this exercise, we will perform spelling correction on a word and a sentence, with the help of the \\'autocorrect\\' library of Python. Follow these steps in order to implement this exercise:Open a Jupyter notebook.Insert a new cell and add the following code to import the necessary libraries:import nltkfrom nltk import word_tokenizefrom autocorrect import spellIn order to correct the spelling of a word, pass a wrongly spelled word as a parameter to the spell() function. Insert a new cell and add the following code to implement this:spell(\\'Natureal\\')\\uf0b7  The code generates the following output:Figure 1.13: Corrected word\\uf0b7  In order to correct the spelling of a sentence, we first need to tokenize it into words. After that, we loop through each word in sentence, autocorrect them, and finally combine them. Insert a new cell and add the following code to implement this:sentence = word_tokenize(\"Ntural Luanguage Processin deals with the art of extracting insightes from Natural Languaes\")We make use of the print() function to print all tokens. Insert a new cell and add the following code to print tokens:print(sentence)\\uf0b7  The code generates the following output:Figure 1.14: Tokens in sentences\\uf0b7  Now that we have got the tokens, we loop through each token in sentence, correct them, and assign them to new variable. Insert a new cell and add the following code to implement this:sentence_corrected = \\' \\'.join([spell(word) for word in sentence])To print the correct sentence, we insert a new cell and add the following code:print(sentence_corrected)The code generates the following output:Figure 1.15: Corrected sentenceIn the preceding figure, we can see that most of the wrongly spelled words have been corrected. But the word \"Processin\" was wrongly converted into \"Procession\". It should have been \"Processing\". It happened because to change \"Processin\" to \"Procession\" or \"Processing,\" an equal number of edits is required. To rectify this, we need to use other kinds of spelling correctors that are aware of context.In the next section, we will look at stemming, which is another form of text normalization.StemmingIn languages such as English, words get transformed into various forms when being used in a sentence. For example, the word \"product\" might get transformed into \"production\" when referring to the process of making something or transformed into \"products\" in plural form. It is necessary to convert these words into their base forms, as they carry the same meaning. Stemming is a process that helps us in doing so. If we look at the following figure, we get a perfect idea about how words get transformed into their base forms:Figure 1.16: Stemming of the word productTo get a better understanding about stemming, we shall look into an exercise in the next section.Exercise 7: StemmingIn this exercise, we will pass a few words through the stemming process such that they get converted into their base forms. Follow these steps to implement this exercise:Open a Jupyter notebook.Insert a new cell and add the following code to import the necessary libraries:import nltkstemmer = nltk.stem.PorterStemmer()Now pass the following words as parameters to the stem() method. To implement this, insert a new cell and add the following code:stemmer.stem(\"production\")When the input is \"production\", the following output is generated:Figure 1.17: Stemmed word for productionstemmer.stem(\"coming\")When the input is \"coming\", the following output is generated:Figure 1.18: Stemmed word for comingstemmer.stem(\"firing\")When the input is \"firing\", the following output is generated:Figure 1.19: Stemmed word for firingstemmer.stem(\"battling\")When the input is \"battling\", the following output is generated:Figure 1.20: Stemmed word for battlingFrom the preceding figures, we can see that the entered words are converted into their base forms.In the next section, we will focus on lemmatization, which is another form of text normalization.LemmatizationSometimes, the stemming process leads to inappropriate results. For example, in the last exercise, the word \"battling\" got transformed to \"battl,\" which has no meaning. To overcome these problems with stemming, we make use of lemmatization. In this process, an additional check is being made, by looking through the dictionary to extract the base form of a word. However, this additional check slows down the process. To get a better understanding about lemmatization, we will look at an exercise in the next section.Exercise 8: Extracting the base word using LemmatizationIn this exercise, we use the lemmatization process to produce the proper form of a given word. Follow these steps to implement this exercise:Open a Jupyter notebook.Insert a new cell and add the following code to import the necessary libraries:import nltknltk.download(\\'wordnet\\')from nltk.stem.wordnet import WordNetLemmatizer\\uf0b7  Create object of the WordNetLemmatizer class. Insert a new cell and add the following code to implement this:lemmatizer = WordNetLemmatizer()Bring the word to its proper form by using the lemmatize() method of the WordNetLemmatizer class. Insert a new cell and add the following code to implement this:lemmatizer.lemmatize(\\'products\\')With the input \"products\", the following output is generated:Figure 1.21: Lemmatized wordlemmatizer.lemmatize(\\'production\\')With the input \"production\", the following output is generated:Figure 1.22: Lemmatized wordlemmatizer.lemmatize(\\'coming\\')With the input \"coming\", the following output is generated:Figure 1.23: Lemmatized wordlemmatizer.lemmatize(\\'battle\\')With the input \"battle\", the following output is generated:Figure 1.24: Lemmatized wordWe have learned how to use the lemmatization process to transform a given word into its base form.In the next section, we will look at another preprocessing step in NLP: named entity recognition (NER).NERNamed entities are usually not present in dictionaries. So, we need to treat them separately. The main objective of this process is to identify the named entities (such as proper nouns) and map them to the categories that are already defined. For example, the categories might include names of persons, places, and so on. To get a better understanding of this process, we\\'ll look at an exercise.Exercise 9: Treating Named EntitiesIn this exercise, we will find out the named entities in a sentence. Follow these steps to implement this exercise:Open a Jupyter notebook.Insert a new cell and add the following code to import the necessary libraries:import nltkfrom nltk import word_tokenizenltk.download(\\'maxent_ne_chunker\\')nltk.download(\\'words\\')\\uf0b7  Declare the sentence variable and assign it a string. Insert a new cell and add the following code to implement this:sentence = \"We are reading a book published by Packt which is based out of Birmingham.\"To find the named entities from the preceding text, insert a new cell and the following code:i = nltk.ne_chunk(nltk.pos_tag(word_tokenize(sentence)), binary=True)[a for a in i if len(a)==1]The code generates the following output:Figure 1.25: Named entityIn the preceding figure, we can see that the code identifies the named entities \"Packt\" and \"Birmingham\" and maps them to an already-defined category such as \"NNP\".In the next section, we will focus on word sense disambiguation, which helps us identify the right sense of any word.Word Sense DisambiguationThere\\'s a popular saying, \"A man is known by the company he keeps\". Similarly, a word\\'s meaning depends on its association with other words in the sentence. This means two or more words with the same spelling may have different meanings in different contexts. This often leads to ambiguity. Word sense disambiguation is the process of mapping a word to the correct sense it carries. We need to disambiguate words based on the sense they carry so that they can be treated as different entities when being analyzed. The following figure displays a perfect example of how ambiguity is caused due to the usage of the same word in different sentences:Figure 1.26: Word sense disambiguitionTo get a better understanding about this process, let\\'s look at an exercise in the next section.Exercise 10: Word Sense DisambiguationIn this exercise, we will find the sense of the word \"bank\" in two different sentences. Follow these steps to implement this exercise:Open a Jupyter notebook.Insert a new cell and add the following code to import the necessary libraries:import nltkfrom nltk.wsd import leskfrom nltk import word_tokenize\\uf0b7  Declare two variables, sentence1 and sentence2, and assign them with appropriate strings. Insert a new cell and the following code to implement this:sentence1 = \"Keep your savings in the bank\"sentence2 = \"It\\'s so risky to drive over the banks of the road\"In order to find the sense of the word \"bank\" in the preceding two sentences, we make use of the lesk algorithm provided by the nltk.wsd library. Insert a new cell and add the following code to implement this:print(lesk(word_tokenize(sentence1), \\'bank\\'))The code generates the following output:Figure 1.27: Sense carried by the word \"bank\" in sentence1Here, savings_bank.n.02 refers to a container for keeping money safely at home. To check the other sense of the word bank, write the following code:print(lesk(word_tokenize(sentence2), \\'bank\\')) The code generates the following output:Figure 1.28: Sense carried by the word \"bank\" in sentence2Here, bank.v.07 refers to a slope in the turn of a road.Thus, with the help of the lesk algorithm, we are able to identify the sense of a word in whatever context. In the next section, we will focus on sentence boundary detection, which helps detect the start and end points of sentences.Sentence Boundary DetectionSentence boundary detection is the method of detecting where one sentence ends and where another sentence begins. If you are thinking that it is pretty easy, as a full stop (.) denotes the end of any sentence and the beginning of another sentence, then you are wrong. This is because there can be instances wherein abbreviations are separated by full stops. Various analyses need to be performed at a sentence level, so detecting boundaries of sentences is essential. An exercise in the next section will provide a better understanding of this process.Exercise 11: Sentence Boundary DetectionIn this exercise, we will extract sentences from a paragraph. Follow these steps to implement this exercise:Open a Jupyter notebook.Insert a new cell and add the following code to import the necessary libraries:import nltkfrom nltk.tokenize import sent_tokenizeWe make use of the sent_tokenize() method to detect sentences in a given text. Insert a new cell and add the following code to implement this:sent_tokenize(\"We are reading a book. Do you know who is the publisher? It is Packt. Packt is based out of Birmingham.\")The code generates the following output:Figure 1.29: List of sentencesAs you can see in the figure, we are able to separate out the sentences from given text.We have covered all the preprocessing steps that are involved in NLP. Now, based on the knowledge we\\'ve gained, we will complete an activity in the next section.Activity 1: Preprocessing of Raw TextWe have a text corpus that is in an improper format. In this activity, we will perform all the pre-processing steps that were discussed earlier to get some meaning out of the text.NoteThe file.txt file can be found at this location: https://bit.ly/2V3ROAa.Follow these steps to implement this activity:Import the necessary libraries.Load the text corpus to a variable.Apply the tokenization process to the text corpus and print the first 20 tokens.Apply spelling correction on each token and print the initial 20 corrected tokens as well as the corrected text corpus.Apply PoS tags to each corrected token and print them.Remove stop words from the corrected token list and print the initial 20 tokens.Apply stemming and lemmatization to the corrected token list and the print initial 20 tokens.Detect the sentence boundaries in the given text corpus and print the total number of sentences. NoteThe solution for this activity can be found on page 254.By now, you should be familiar with what NLP is and what basic pre-processing steps are needed to carry out any NLP project. In the next section, we will focus on different phases that are included in an NLP project.\\xa0 Kick Starting an NLP ProjectWe can divide an NLP project into several sub-projects or phases. These phases are followed sequentially. This tends to increase the overall efficiency of the process as each phase is generally carried out by specialized resources. An NLP project has to go through six major phases, which are outlined in the following figure:Figure 1.30: Phases of an NLP projectSuppose you are working on a project in which you need to collect tweets and analyze their sentiments. We will explain how this is carried out by discussing each phase in the coming section.Data CollectionThis is the initial phase of any NLP project. Our sole purpose is to collect data as per our requirements. For this, we may either use existing data, collect data from various online repositories, or create our own dataset by crawling the web. In our case, we will collect tweets.Data PreprocessingOnce the data is collected, we need to clean it. For the process of cleaning, we make use of the different pre-processing steps that we have used in this chapter. It is necessary to clean the collected data, as dirty data tends to reduce effectiveness and accuracy. In our case, we will remove the unnecessary URLs, words, and more from the collected tweets.Feature ExtractionComputers understand only binary digits: 0 and 1. Thus, every instruction we feed into a computer gets transformed into binary digits. Similarly, machine learning models tend to understand only numeric data. As such, it becomes necessary to convert the text data into its equivalent numerical form. In our case, we represent the cleaned tweets using different kinds of matrices, such as bag of words and TF-IDF. We will be learning more about these matrices in later chapters.Model DevelopmentOnce the feature set is ready, we need to develop a suitable model that can be trained to gain knowledge from the data. These models are generally statistical, machine learning-based, deep learning-based, or reinforcement learning-based. In our case, we will build a model that is capable of extracting sentiments from numeric matrices.Model AssessmentAfter developing a model, it is essential to benchmark it. This process of benchmarking is known as model assessment. In this step, we will evaluate the performance of our model by comparing it to others. This can be done by using different parameters or metrics. These parameters include F1, precision, recall, and accuracy. In our case, we will evaluate the newly created model by checking how well it performs when extracting the sentiments of the tweets.Model DeploymentThis is the final stage for most industrial NLP projects. In this stage, the models are put into production. They are either integrated into an existing system or new products are created by keeping this model as a base. In our case, we will deploy our model to production, such that it can extract sentiments from tweets in real time.\\xa0 SummaryIn this chapter, we learned how NLP is different from text analytics. We also covered the various pre-processing steps that are included in NLP. We looked at the different phases an NLP project has to pass through. In the next chapter, you will learn about the different methods required for extracting features from unstructured texts, such as TF-IDF and bag of words. You will also learn about NLP tasks such as tokenization, lemmatization, and stemming in more detail. Furthermore, text visualization techniques such as word clouds will be introduced.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
